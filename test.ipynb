{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d73644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib3\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from meteostat import Point, Hourly, units\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "##### STRAVA API DATA EXTRACTION ####\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "auth_url = 'https://www.strava.com/oauth/token'\n",
    "\n",
    "payload = {\n",
    "    'client_id': st.secrets['client_id'],\n",
    "    'client_secret': st.secrets['client_secret'],\n",
    "    'refresh_token': st.secrets['refresh_token'],\n",
    "    'grant_type': 'refresh_token',\n",
    "    'f': 'json'\n",
    "}\n",
    "\n",
    "res = requests.post(auth_url, data=payload, verify=False)\n",
    "access_token = res.json()['access_token']\n",
    "\n",
    "header = {'Authorization': 'Bearer ' + access_token}\n",
    "\n",
    "def get_strava_data() -> pd.DataFrame:\n",
    "    '''This function builds the dataframe from Strava API data. It is used to then cache the dataframe for faster loading in the Streamlit app.\n",
    "    \n",
    "    Returns:\n",
    "        pre_df (DataFrame): DataFrame of activities and gear data'''\n",
    "        \n",
    "    # Strava API only allows 200 results per page. This function loops through until all results are collected\n",
    "    def get_activities_data() -> pd.DataFrame:\n",
    "        '''This function gets all activities data from Strava API\n",
    "        \n",
    "        Returns:\n",
    "            data (DataFrame): Normalized JSON data of activities'''\n",
    "            \n",
    "        # set the URL for the Strava API\n",
    "        activities_url = 'https://www.strava.com/api/v3/athlete/activities'\n",
    "        # set value of page to start at page 1\n",
    "        page = 1\n",
    "        # create an empty list to store all data\n",
    "        data = []\n",
    "        # set new_results to True to start the loop\n",
    "        new_results = True\n",
    "        \n",
    "        while new_results:\n",
    "            # requests one page at a time (200 results)\n",
    "            get_activities = requests.get(activities_url, headers=header, params={'per_page': 200, 'page': page}).json()\n",
    "            # feedback\n",
    "            print(f\"Fetching page {page}\")\n",
    "            print(f\"Number of activities fetched: {len(get_activities)}\")\n",
    "            # if there are no results, the loop will stop\n",
    "            new_results = get_activities\n",
    "            # add the results to the data list\n",
    "            data.extend(get_activities)\n",
    "            # increment the page number\n",
    "            page += 1\n",
    "\n",
    "            if page > 20:\n",
    "                print('Stopping after 20 pages to avoid excessive API calls')\n",
    "                # TODO add backup csv file to load if the API breaks\n",
    "                break\n",
    "            \n",
    "        return pd.read_pickle('./data/activity_data_backup.pkl')\n",
    "            \n",
    "    # get all activities data\n",
    "    activities = get_activities_data()\n",
    "    \n",
    "    # convert meters to miles\n",
    "    activities.distance = (activities.distance / 1609.34).round(2)\n",
    "    # convert to mph\n",
    "    activities.average_speed = (activities.average_speed * 2.23694).round(2)\n",
    "    activities.max_speed = (activities.max_speed * 2.23694).round(2)\n",
    "    # convert to feet\n",
    "    activities.total_elevation_gain = (activities.total_elevation_gain * 3.28084).round(2)\n",
    "    activities.elev_high = (activities.elev_high * 3.28084).round(2)\n",
    "    activities.elev_low = (activities.elev_low * 3.28084).round(2)\n",
    "\n",
    "    activities_df = pd.DataFrame(activities)\n",
    "    \n",
    "    def add_weather_data(df: pd.DataFrame, max_workers=30) -> pd.DataFrame:\n",
    "        '''This function gets weather data from Meteostat and adds it onto the activities DataFrame\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): Activities data frame that uses latitude, longitude, and timestamps to get weather data\n",
    "            max_worker (int): Number of threads to use in the multi-threading process\n",
    "            \n",
    "        Returns:\n",
    "            df (DataFrame): Original df with weatehr data appended'''\n",
    "            \n",
    "        def get_weather(row):\n",
    "            '''This function takes the latitude, longitude, and timestamp for each row and calls the Meteostat API for data\n",
    "            \n",
    "            Args:\n",
    "                row: The row in the DataFrame used in the parent function\n",
    "                \n",
    "            Returns:\n",
    "                weather_data (dict): The temperature and relative humidity of the row's activity as a dictionary'''\n",
    "            \n",
    "            # get the location of the activity\n",
    "            location = Point(row['start_latitude'], row['start_longitude'])\n",
    "            # get the time of the activity\n",
    "            timestamp = pd.to_datetime(row['start_date_local'])\n",
    "            # only use the hour it started\n",
    "            start = end = timestamp.replace(tzinfo=None, minute=0, second=0, microsecond=0)\n",
    "\n",
    "            # call meteostat API\n",
    "            try:\n",
    "                data = Hourly(location, start, end)\n",
    "                data = data.convert(units.imperial).fetch()\n",
    "                if not data.empty:\n",
    "                    # only get the first row of data\n",
    "                    weather = data[['temp', 'rhum']].iloc[0]\n",
    "                    return {'temp': weather['temp'], 'rhum': weather['rhum']}\n",
    "                else:\n",
    "                    return {'temp': None, 'rhum': None}\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching weather for {timestamp}: {e}\")\n",
    "                return {'temp': None, 'rhum': None}\n",
    "            \n",
    "        # separate the latitude and longitude from the activity data\n",
    "        df[['start_latitude', 'start_longitude']] = pd.DataFrame(df['start_latlng'].tolist(), index=df.index)\n",
    "\n",
    "        # multi-threading so the function can call the API and iterate through rows faster\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            weather_data = list(executor.map(get_weather, [row for _, row in df.iterrows()]))\n",
    "\n",
    "        # get the weatehr data and concat the two DataFrames\n",
    "        weather_df = pd.DataFrame(weather_data)\n",
    "        Hourly.clear_cache()\n",
    "        return pd.concat([df.reset_index(drop=True), weather_df.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    activities_df = add_weather_data(activities_df)\n",
    "\n",
    "    # get distinct gear id's\n",
    "    gear_id_list = activities_df['gear_id'].unique()\n",
    "    gear_id_list = gear_id_list[~pd.isnull(gear_id_list)]\n",
    "\n",
    "    def get_gear_data(gear_list: list) -> pd.DataFrame:\n",
    "        '''This function gets gear data from Strava API\n",
    "        \n",
    "        Args:\n",
    "            gear_list (array): List of distinct gear ids\n",
    "            \n",
    "            Returns:\n",
    "                data (DataFrame): Normalized JSON data of gear'''\n",
    "        # set the URL for the Strava API\n",
    "        gear_url = 'https://www.strava.com/api/v3/gear/{id}'\n",
    "        # create empty list to store gear data\n",
    "        data = []\n",
    "        # loop through gear_list and get gear data\n",
    "        for gear_id in gear_list:\n",
    "            get_gear = requests.get(gear_url.format(id=gear_id), headers=header).json()\n",
    "            data.append(get_gear)\n",
    "        return pd.json_normalize(data)\n",
    "    \n",
    "    # get all gear data\n",
    "    gear = get_gear_data(gear_id_list)\n",
    "\n",
    "    # convert meters to miles\n",
    "    gear.distance = gear.distance / 1609.34\n",
    "\n",
    "    gear = gear.drop(columns=['converted_distance'])\n",
    "\n",
    "    ##### DATA CLEANING AND TRANSFORMATION #####\n",
    "    # create base dataframe joining activity and gear data\n",
    "    pre_df = pd.merge(activities_df,\n",
    "                    gear, \n",
    "                    how='left',\n",
    "                    left_on='gear_id',\n",
    "                    right_on='id',\n",
    "                    suffixes=('_activity', '_gear')).drop(columns='id_gear')\n",
    "\n",
    "    # convert moving_time and elapsed time to H% M% S% format\n",
    "    pre_df['moving_time'] = pd.to_timedelta(pd.to_datetime(pre_df['moving_time'], unit='s').dt.strftime('%H:%M:%S'))\n",
    "    pre_df['elapsed_time'] = pd.to_timedelta(pd.to_datetime(pre_df['elapsed_time'], unit='s').dt.strftime('%H:%M:%S'))\n",
    "\n",
    "    # convert start_date and start_date_local to datetime\n",
    "    pre_df['start_date'] = pd.to_datetime(pd.to_datetime(pre_df['start_date']).dt.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    pre_df['start_date_local'] = pd.to_datetime(pd.to_datetime(pre_df['start_date_local']).dt.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    # add start time for analysis and in am/pm format\n",
    "    pre_df['start_time_local_24h'] = pd.to_datetime(pre_df['start_date_local']).dt.time\n",
    "    pre_df['start_time_local_12h'] = pd.to_datetime(pre_df['start_date_local']).dt.strftime(\"%I:%M %p\")\n",
    "\n",
    "    # add day of week\n",
    "    pre_df['day_of_week'] = pd.to_datetime(pre_df['start_date_local']).dt.day_name()\n",
    "    pre_df['weekday'] = pd.to_datetime(pre_df['start_date_local']).dt.weekday\n",
    "\n",
    "    # add month\n",
    "    pre_df['month'] = pd.to_datetime(pre_df['start_date_local']).dt.month_name()\n",
    "    pre_df['month_num'] = pd.to_datetime(pre_df['start_date_local']).dt.month\n",
    "\n",
    "    # add month year\n",
    "    pre_df['month_year'] = pd.to_datetime(pd.to_datetime(pre_df['start_date_local']).dt.strftime('%Y-%m'))\n",
    "    \n",
    "    # add month year name\n",
    "    pre_df['month_year_name'] = pd.to_datetime(pre_df['start_date_local']).dt.strftime('%b %Y')\n",
    "\n",
    "    # add year label\n",
    "    pre_df['year'] = pd.to_datetime(pre_df['start_date_local']).dt.year\n",
    "    \n",
    "    pre_df.drop(columns=['start_latlng', 'end_latlng', 'start_latitude', 'start_longitude'], inplace=True)\n",
    "    \n",
    "    return pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4447bbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1\n",
      "Number of activities fetched: 200\n",
      "Fetching page 2\n",
      "Number of activities fetched: 200\n",
      "Fetching page 3\n",
      "Number of activities fetched: 200\n",
      "Fetching page 4\n",
      "Number of activities fetched: 25\n",
      "Fetching page 5\n",
      "Number of activities fetched: 0\n"
     ]
    }
   ],
   "source": [
    "data = get_strava_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "600c2774",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'month_num'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m/var/folders/nv/clclcv110f3850cs6h38c_ww0000gn/T/ipykernel_18252/547929831.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m test_df = data.query(\u001b[33m'type in \"Run\" and year == 2025'\u001b[39m).sort_values(by=\u001b[33m'month_num'\u001b[39m).groupby([\u001b[33m'month'\u001b[39m, \u001b[33m'type'\u001b[39m], sort=\u001b[38;5;28;01mFalse\u001b[39;00m).agg(Activities=(\u001b[33m'upload_id'\u001b[39m, \u001b[33m'count'\u001b[39m)).reset_index().rename(columns={\u001b[33m'month'\u001b[39m: \u001b[33m'Month'\u001b[39m, \u001b[33m'type'\u001b[39m: \u001b[33m'Activity Type'\u001b[39m})\n",
      "\u001b[32m~/VSCode/tom_runs_the_world/.venv/lib/python3.13/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[39m\n\u001b[32m   7185\u001b[39m             )\n\u001b[32m   7186\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m len(by):\n\u001b[32m   7187\u001b[39m             \u001b[38;5;66;03m# len(by) == 1\u001b[39;00m\n\u001b[32m   7188\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m7189\u001b[39m             k = self._get_label_or_level_values(by[\u001b[32m0\u001b[39m], axis=axis)\n\u001b[32m   7190\u001b[39m \n\u001b[32m   7191\u001b[39m             \u001b[38;5;66;03m# need to rewrap column in Series to apply key function\u001b[39;00m\n\u001b[32m   7192\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m~/VSCode/tom_runs_the_world/.venv/lib/python3.13/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'month_num'"
     ]
    }
   ],
   "source": [
    "test_df = data.query('type in \"Run\" and year == 2025').sort_values(by='month_num').groupby(['month', 'type'], sort=False).agg(Activities=('upload_id', 'count')).reset_index().rename(columns={'month': 'Month', 'type': 'Activity Type'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ffaeb3",
   "metadata": {},
   "source": [
    "## Activity backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6c561c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activities_data() -> pd.DataFrame:\n",
    "    '''This function gets all activities data from Strava API\n",
    "    \n",
    "    Returns:\n",
    "        data (DataFrame): Normalized JSON data of activities'''\n",
    "        \n",
    "    # set the URL for the Strava API\n",
    "    activities_url = 'https://www.strava.com/api/v3/athlete/activities'\n",
    "    # set value of page to start at page 1\n",
    "    page = 1\n",
    "    # create an empty list to store all data\n",
    "    data = []\n",
    "    # set new_results to True to start the loop\n",
    "    new_results = True\n",
    "    \n",
    "    while new_results:\n",
    "        # requests one page at a time (200 results)\n",
    "        get_activities = requests.get(activities_url, headers=header, params={'per_page': 200, 'page': page}).json()\n",
    "        # feedback\n",
    "        print(f\"Fetching page {page}\")\n",
    "        print(f\"Number of activities fetched: {len(get_activities)}\")\n",
    "        # if there are no results, the loop will stop\n",
    "        new_results = get_activities\n",
    "        # add the results to the data list\n",
    "        data.extend(get_activities)\n",
    "        # increment the page number\n",
    "        page += 1\n",
    "\n",
    "        if page > 20:\n",
    "            print('Stopping after 20 pages to avoid excessive API calls')\n",
    "            # TODO add backup csv file to load if the API breaks\n",
    "            break\n",
    "        \n",
    "    return pd.json_normalize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f82eb1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1\n",
      "Number of activities fetched: 200\n",
      "Fetching page 2\n",
      "Number of activities fetched: 200\n",
      "Fetching page 3\n",
      "Number of activities fetched: 200\n",
      "Fetching page 4\n",
      "Number of activities fetched: 25\n",
      "Fetching page 5\n",
      "Number of activities fetched: 0\n"
     ]
    }
   ],
   "source": [
    "backup_activity_data = get_activities_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "75abbc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_activity_data.to_pickle('data/activity_data_backup.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
