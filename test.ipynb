{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29d73644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib3\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from meteostat import Point, Hourly, units\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "##### STRAVA API DATA EXTRACTION ####\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "auth_url = 'https://www.strava.com/oauth/token'\n",
    "\n",
    "payload = {\n",
    "    'client_id': st.secrets['client_id'],\n",
    "    'client_secret': st.secrets['client_secret'],\n",
    "    'refresh_token': st.secrets['refresh_token'],\n",
    "    'grant_type': 'refresh_token',\n",
    "    'f': 'json'\n",
    "}\n",
    "\n",
    "res = requests.post(auth_url, data=payload, verify=False)\n",
    "access_token = res.json()['access_token']\n",
    "\n",
    "header = {'Authorization': 'Bearer ' + access_token}\n",
    "\n",
    "def get_strava_data() -> pd.DataFrame:\n",
    "    '''This function builds the dataframe from Strava API data. It is used to then cache the dataframe for faster loading in the Streamlit app.\n",
    "    \n",
    "    Returns:\n",
    "        pre_df (DataFrame): DataFrame of activities and gear data'''\n",
    "        \n",
    "    # Strava API only allows 200 results per page. This function loops through until all results are collected\n",
    "    def get_activities_data() -> pd.DataFrame:\n",
    "        '''This function gets all activities data from Strava API\n",
    "        \n",
    "        Returns:\n",
    "            data (DataFrame): Normalized JSON data of activities'''\n",
    "            \n",
    "        # set the URL for the Strava API\n",
    "        activities_url = 'https://www.strava.com/api/v3/athlete/activities'\n",
    "        # set value of page to start at page 1\n",
    "        page = 1\n",
    "        # create an empty list to store all data\n",
    "        data = []\n",
    "        # set new_results to True to start the loop\n",
    "        new_results = True\n",
    "        \n",
    "        st.write('Fetching Activities...')\n",
    "        \n",
    "        while new_results:\n",
    "            # requests one page at a time (200 results)\n",
    "            get_activities = requests.get(activities_url, headers=header, params={'per_page': 200, 'page': page}).json()\n",
    "            # feedback\n",
    "            print(f\"Fetching page {page}\")\n",
    "            print(f\"Number of activities fetched: {len(get_activities)}\")\n",
    "            # if there are no results, the loop will stop\n",
    "            new_results = get_activities\n",
    "            # add the results to the data list\n",
    "            data.extend(get_activities)\n",
    "            # increment the page number\n",
    "            page += 1\n",
    "\n",
    "            if page > 20:\n",
    "                print('Stopping after 20 pages to avoid excessive API calls.')\n",
    "                \n",
    "                return None\n",
    "            \n",
    "        return pd.json_normalize(data)\n",
    "            \n",
    "    # get all activities data\n",
    "    activities = get_activities_data()\n",
    "    \n",
    "    if activities.empty:\n",
    "        return pd.read_pickle('./data/full_data_backup.pkl')\n",
    "    \n",
    "    # convert meters to miles\n",
    "    activities.distance = (activities.distance / 1609.34).round(2)\n",
    "    # convert to mph\n",
    "    activities.average_speed = (activities.average_speed * 2.23694).round(2)\n",
    "    activities.max_speed = (activities.max_speed * 2.23694).round(2)\n",
    "    # convert to feet\n",
    "    activities.total_elevation_gain = (activities.total_elevation_gain * 3.28084).round(2)\n",
    "    activities.elev_high = (activities.elev_high * 3.28084).round(2)\n",
    "    activities.elev_low = (activities.elev_low * 3.28084).round(2)\n",
    "\n",
    "    activities_df = pd.DataFrame(activities)\n",
    "    \n",
    "    def add_weather_data(df: pd.DataFrame, max_workers=30) -> pd.DataFrame:\n",
    "        '''This function gets weather data from Meteostat and adds it onto the activities DataFrame\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): Activities data frame that uses latitude, longitude, and timestamps to get weather data\n",
    "            max_worker (int): Number of threads to use in the multi-threading process\n",
    "            \n",
    "        Returns:\n",
    "            df (DataFrame): Original df with weatehr data appended'''\n",
    "            \n",
    "        def get_weather(row):\n",
    "            '''This function takes the latitude, longitude, and timestamp for each row and calls the Meteostat API for data\n",
    "            \n",
    "            Args:\n",
    "                row: The row in the DataFrame used in the parent function\n",
    "                \n",
    "            Returns:\n",
    "                weather_data (dict): The temperature and relative humidity of the row's activity as a dictionary'''\n",
    "            \n",
    "            # get the location of the activity\n",
    "            location = Point(row['start_latitude'], row['start_longitude'])\n",
    "            # get the time of the activity\n",
    "            timestamp = pd.to_datetime(row['start_date_local'])\n",
    "            # only use the hour it started\n",
    "            start = end = timestamp.replace(tzinfo=None, minute=0, second=0, microsecond=0)\n",
    "\n",
    "            # call meteostat API\n",
    "            try:\n",
    "                data = Hourly(location, start, end)\n",
    "                data = data.convert(units.imperial).fetch()\n",
    "                if not data.empty:\n",
    "                    # only get the first row of data\n",
    "                    weather = data[['temp', 'rhum']].iloc[0]\n",
    "                    return {'temp': weather['temp'], 'rhum': weather['rhum']}\n",
    "                else:\n",
    "                    return {'temp': None, 'rhum': None}\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching weather for {timestamp}: {e}\")\n",
    "                return {'temp': None, 'rhum': None}\n",
    "            \n",
    "        # separate the latitude and longitude from the activity data\n",
    "        df[['start_latitude', 'start_longitude']] = pd.DataFrame(df['start_latlng'].tolist(), index=df.index)\n",
    "\n",
    "        # multi-threading so the function can call the API and iterate through rows faster\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            weather_data = list(executor.map(get_weather, [row for _, row in df.iterrows()]))\n",
    "\n",
    "        # get the weatehr data and concat the two DataFrames\n",
    "        weather_df = pd.DataFrame(weather_data)\n",
    "        Hourly.clear_cache()\n",
    "        return pd.concat([df.reset_index(drop=True), weather_df.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    activities_df = add_weather_data(activities_df)\n",
    "\n",
    "    # get distinct gear id's\n",
    "    gear_id_list = activities_df['gear_id'].unique()\n",
    "    gear_id_list = gear_id_list[~pd.isnull(gear_id_list)]\n",
    "\n",
    "    def get_gear_data(gear_list: list) -> pd.DataFrame:\n",
    "        '''This function gets gear data from Strava API\n",
    "        \n",
    "        Args:\n",
    "            gear_list (array): List of distinct gear ids\n",
    "            \n",
    "            Returns:\n",
    "                data (DataFrame): Normalized JSON data of gear'''\n",
    "        # set the URL for the Strava API\n",
    "        gear_url = 'https://www.strava.com/api/v3/gear/{id}'\n",
    "        # create empty list to store gear data\n",
    "        data = []\n",
    "        # loop through gear_list and get gear data\n",
    "        for gear_id in gear_list:\n",
    "            get_gear = requests.get(gear_url.format(id=gear_id), headers=header).json()\n",
    "            data.append(get_gear)\n",
    "        return pd.json_normalize(data)\n",
    "    \n",
    "    # get all gear data\n",
    "    gear = get_gear_data(gear_id_list)\n",
    "\n",
    "    # convert meters to miles\n",
    "    gear.distance = gear.distance / 1609.34\n",
    "\n",
    "    gear = gear.drop(columns=['converted_distance'])\n",
    "\n",
    "    ##### DATA CLEANING AND TRANSFORMATION #####\n",
    "    # create base dataframe joining activity and gear data\n",
    "    pre_df = pd.merge(activities_df,\n",
    "                    gear, \n",
    "                    how='left',\n",
    "                    left_on='gear_id',\n",
    "                    right_on='id',\n",
    "                    suffixes=('_activity', '_gear')).drop(columns='id_gear')\n",
    "\n",
    "    # convert moving_time and elapsed time to H% M% S% format\n",
    "    pre_df['moving_time'] = pd.to_timedelta(pd.to_datetime(pre_df['moving_time'], unit='s').dt.strftime('%H:%M:%S'))\n",
    "    pre_df['elapsed_time'] = pd.to_timedelta(pd.to_datetime(pre_df['elapsed_time'], unit='s').dt.strftime('%H:%M:%S'))\n",
    "\n",
    "    # convert start_date and start_date_local to datetime\n",
    "    pre_df['start_date'] = pd.to_datetime(pd.to_datetime(pre_df['start_date']).dt.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    pre_df['start_date_local'] = pd.to_datetime(pd.to_datetime(pre_df['start_date_local']).dt.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    # add start time for analysis and in am/pm format\n",
    "    pre_df['start_time_local_24h'] = pd.to_datetime(pre_df['start_date_local']).dt.time\n",
    "    pre_df['start_time_local_12h'] = pd.to_datetime(pre_df['start_date_local']).dt.strftime(\"%I:%M %p\")\n",
    "\n",
    "    # add day of week\n",
    "    pre_df['weekday'] = pd.to_datetime(pre_df['start_date_local']).dt.day_name()\n",
    "    pre_df['weekday_num'] = pd.to_datetime(pre_df['start_date_local']).dt.weekday\n",
    "\n",
    "    # add month\n",
    "    pre_df['month'] = pd.to_datetime(pre_df['start_date_local']).dt.month_name()\n",
    "    pre_df['month_num'] = pd.to_datetime(pre_df['start_date_local']).dt.month\n",
    "    pre_df['monthly_date'] = pd.to_datetime(pd.to_datetime(pre_df['start_date_local']).dt.strftime('%Y-%m')).apply(lambda x: x.replace(year=2025))\n",
    "\n",
    "    # add month year\n",
    "    pre_df['month_year'] = pd.to_datetime(pd.to_datetime(pre_df['start_date_local']).dt.strftime('%Y-%m'))\n",
    "    \n",
    "    # add month year name\n",
    "    pre_df['month_year_name'] = pd.to_datetime(pre_df['start_date_local']).dt.strftime('%b %Y')\n",
    "\n",
    "    # add year label\n",
    "    pre_df['year'] = pd.to_datetime(pre_df['start_date_local']).dt.year\n",
    "    \n",
    "    pre_df.drop(columns=['start_latlng', 'end_latlng', 'start_latitude', 'start_longitude'], inplace=True)\n",
    "    \n",
    "    return pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4447bbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 23:50:20.243 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-20 23:50:20.274 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/tomlaporte/VSCode/tom_runs_the_world/.venv/lib/python3.13/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-04-20 23:50:20.275 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-20 23:50:20.275 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-20 23:50:20.275 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1\n",
      "Number of activities fetched: 200\n",
      "Fetching page 2\n",
      "Number of activities fetched: 200\n",
      "Fetching page 3\n",
      "Number of activities fetched: 200\n",
      "Fetching page 4\n",
      "Number of activities fetched: 26\n",
      "Fetching page 5\n",
      "Number of activities fetched: 0\n"
     ]
    }
   ],
   "source": [
    "data = get_strava_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "977318a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Sunday\n",
       "1      Saturday\n",
       "2        Friday\n",
       "3        Friday\n",
       "4        Monday\n",
       "         ...   \n",
       "621      Friday\n",
       "622    Thursday\n",
       "623      Friday\n",
       "624      Sunday\n",
       "625      Sunday\n",
       "Name: weekday, Length: 626, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['weekday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ceee4813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        6-Sunday\n",
       "1      5-Saturday\n",
       "2        4-Friday\n",
       "3        4-Friday\n",
       "4        0-Monday\n",
       "          ...    \n",
       "621      4-Friday\n",
       "622    3-Thursday\n",
       "623      4-Friday\n",
       "624      6-Sunday\n",
       "625      6-Sunday\n",
       "Length: 626, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['weekday_num'].astype(str) + '-' + data['weekday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34c5f500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2025-04-19 19:46:55\n",
       "1     2025-04-18 19:32:51\n",
       "2     2025-04-18 15:26:21\n",
       "3     2025-04-14 18:31:35\n",
       "4     2025-04-13 17:38:21\n",
       "              ...        \n",
       "620   2025-08-07 17:02:13\n",
       "621   2025-08-06 18:41:31\n",
       "622   2025-07-31 18:38:24\n",
       "623   2025-05-03 10:42:38\n",
       "624   2025-05-03 10:32:53\n",
       "Name: monthly_date, Length: 625, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['monthly_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "600c2774",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = data.query('type in \"Run\" and year == 2025').sort_values(by='month_num').groupby(['month', 'type'], sort=False).agg(Activities=('upload_id', 'count')).reset_index().rename(columns={'month': 'Month', 'type': 'Activity Type'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9948de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Month",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Activity Type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Activities",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "da4d4eb3-3cf5-4841-ab93-77f02e06e2e1",
       "rows": [
        [
         "0",
         "January",
         "Run",
         "10"
        ],
        [
         "1",
         "February",
         "Run",
         "7"
        ],
        [
         "2",
         "March",
         "Run",
         "11"
        ],
        [
         "3",
         "April",
         "Run",
         "6"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Activity Type</th>\n",
       "      <th>Activities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>January</td>\n",
       "      <td>Run</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>February</td>\n",
       "      <td>Run</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>March</td>\n",
       "      <td>Run</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>April</td>\n",
       "      <td>Run</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Month Activity Type  Activities\n",
       "0   January           Run          10\n",
       "1  February           Run           7\n",
       "2     March           Run          11\n",
       "3     April           Run           6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ffaeb3",
   "metadata": {},
   "source": [
    "## Activity backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c561c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activities_data() -> pd.DataFrame:\n",
    "    '''This function gets all activities data from Strava API\n",
    "    \n",
    "    Returns:\n",
    "        data (DataFrame): Normalized JSON data of activities'''\n",
    "        \n",
    "    # set the URL for the Strava API\n",
    "    activities_url = 'https://www.strava.com/api/v3/athlete/activities'\n",
    "    # set value of page to start at page 1\n",
    "    page = 1\n",
    "    # create an empty list to store all data\n",
    "    data = []\n",
    "    # set new_results to True to start the loop\n",
    "    new_results = True\n",
    "    \n",
    "    while new_results:\n",
    "        # requests one page at a time (200 results)\n",
    "        get_activities = requests.get(activities_url, headers=header, params={'per_page': 200, 'page': page}).json()\n",
    "        # feedback\n",
    "        print(f\"Fetching page {page}\")\n",
    "        print(f\"Number of activities fetched: {len(get_activities)}\")\n",
    "        # if there are no results, the loop will stop\n",
    "        new_results = get_activities\n",
    "        # add the results to the data list\n",
    "        data.extend(get_activities)\n",
    "        # increment the page number\n",
    "        page += 1\n",
    "\n",
    "        if page > 20:\n",
    "            print('Stopping after 20 pages to avoid excessive API calls')\n",
    "            # TODO add backup csv file to load if the API breaks\n",
    "            break\n",
    "        \n",
    "    return pd.json_normalize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f82eb1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1\n",
      "Number of activities fetched: 200\n",
      "Fetching page 2\n",
      "Number of activities fetched: 200\n",
      "Fetching page 3\n",
      "Number of activities fetched: 200\n",
      "Fetching page 4\n",
      "Number of activities fetched: 25\n",
      "Fetching page 5\n",
      "Number of activities fetched: 0\n"
     ]
    }
   ],
   "source": [
    "backup_activity_data = get_activities_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "75abbc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_activity_data.to_pickle('data/activity_data_backup.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
